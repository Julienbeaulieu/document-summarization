{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Convert to lowercase and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_em(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1, if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if  num_same ==0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "sys1 = 'the cat was found under the bed'\n",
    "hum1 = 'the cat was under the bed, ey mon tabarnque il se passe quoi'\n",
    "sys2 = 'the tiny little cat was found under the big funny bed'\n",
    "sys3 = 'oh hello Mr the cat, I was under the chair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846153"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(hum1, sys2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scorer.score(hum1, sys2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.5454545454545454, recall=0.46153846153846156, fmeasure=0.4999999999999999),\n",
       " 'rouge2': Score(precision=0.2, recall=0.16666666666666666, fmeasure=0.1818181818181818),\n",
       " 'rougeL': Score(precision=0.5454545454545454, recall=0.46153846153846156, fmeasure=0.4999999999999999)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411764"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [hum1, hum1, hum1]\n",
    "preds = [sys1, sys2, sys3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [scorer.score(target, pred) for target, pred in zip(targets, preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge1': Score(precision=0.8571428571428571, recall=0.46153846153846156, fmeasure=0.6),\n",
       "  'rouge2': Score(precision=0.6666666666666666, recall=0.3333333333333333, fmeasure=0.4444444444444444),\n",
       "  'rougeL': Score(precision=0.8571428571428571, recall=0.46153846153846156, fmeasure=0.6)},\n",
       " {'rouge1': Score(precision=0.5454545454545454, recall=0.46153846153846156, fmeasure=0.4999999999999999),\n",
       "  'rouge2': Score(precision=0.2, recall=0.16666666666666666, fmeasure=0.1818181818181818),\n",
       "  'rougeL': Score(precision=0.5454545454545454, recall=0.46153846153846156, fmeasure=0.4999999999999999)},\n",
       " {'rouge1': Score(precision=0.5, recall=0.38461538461538464, fmeasure=0.4347826086956522),\n",
       "  'rouge2': Score(precision=0.3333333333333333, recall=0.25, fmeasure=0.28571428571428575),\n",
       "  'rougeL': Score(precision=0.5, recall=0.38461538461538464, fmeasure=0.4347826086956522)}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_f1, rouge2_f1, rougeL_f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "for score in scores:\n",
    "    for k, v in score.items():\n",
    "        if k == 'rouge1':\n",
    "            rouge1_f1 += v.fmeasure\n",
    "        if k == 'rouge2':\n",
    "            rouge2_f1 += v.fmeasure\n",
    "        if k == 'rougeL':\n",
    "            rougeL_f1 += v.fmeasure\n",
    "            \n",
    "eval_dict = {\n",
    "             'rouge1': rouge1_f1 / len(scores),\n",
    "             'rouge2': rouge2_f1 / len(scores),\n",
    "             'rougeL': rougeL_f1 / len(scores)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.5115942028985506,\n",
       " 'rouge2': 0.303992303992304,\n",
       " 'rougeL': 0.5115942028985506}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (summarization)",
   "language": "python",
   "name": "summarization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
